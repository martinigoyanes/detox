{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Midgard/home/martinig/miniconda3/envs/detox/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "from gedi_adapter import GediAdapter\n",
    "import text_processing\n",
    "\n",
    "# TODO:\n",
    "# Use fine-tuned paraphraser on toxic parallel data\n",
    "# Finetune ROBERTA on toxic dataset to use a classifier for the re-rank step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at s-nlp/gpt2-base-gedi-detoxification were not used when initializing GPT2LMHeadModel: ['logit_scale', 'bias']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "para_name = 'ceshine/t5-paraphrase-paws-msrp-opinosis'\n",
    "gedi_path = 's-nlp/gpt2-base-gedi-detoxification'\n",
    "clf_name = 'SkolkovoInstitute/roberta_toxicity_classifier_v1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(para_name)\n",
    "para_model = AutoModelForSeq2SeqLM.from_pretrained(para_name)\n",
    "gedi_model = AutoModelForCausalLM.from_pretrained(gedi_path)\n",
    "\n",
    "# clf = RobertaForSequenceClassification.from_pretrained(clf_name)\n",
    "# clf_tokenizer = RobertaTokenizer.from_pretrained(clf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias: tensor([[ 0.0844, -0.0844]]), logit_scale: tensor([[1.2702]])\n"
     ]
    }
   ],
   "source": [
    "## Setup\n",
    "para_model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "# add gedi-specific parameters\n",
    "if os.path.exists(gedi_path):\n",
    "    w = torch.load(gedi_path + '/pytorch_model.bin', map_location='cpu')\n",
    "    gedi_model.bias = w['bias']\n",
    "    gedi_model.logit_scale = w['logit_scale']\n",
    "    del w\n",
    "else:\n",
    "    gedi_model.bias = torch.tensor([[ 0.08441592, -0.08441573]])\n",
    "    gedi_model.logit_scale = torch.tensor([[1.2701858]])\n",
    "print(f\"bias: {gedi_model.bias}, logit_scale: {gedi_model.logit_scale}\")\n",
    "\n",
    "NEW_POS = tokenizer.encode('normal', add_special_tokens=False)[0]\n",
    "NEW_NEG = tokenizer.encode('toxic', add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "with open('../../data/test_10k_toxic', 'r') as f:\n",
    "    test_toxic_data = [line.strip() for line in f.readlines()]\n",
    "print(len(test_toxic_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you are a fucking idiot .', 'as stupid and arrogant as his boss', 'a stupid society does stupid things and votes for stupid politicians .', 'a president who is an idiot .', 'colbert is a stupid moron and is a terrorist .']\n",
      "CPU times: user 2min 8s, sys: 7.71 s, total: 2min 16s\n",
      "Wall time: 17.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I've been thinking about this a lot lately, and I think it's time\",\n",
       " 'As aroungeant and snooty, as his boss,',\n",
       " 'A bad society does the wrong things, and votes for the wrong politicians.',\n",
       " 'A president who is an Iroquois is not a president.',\n",
       " 'Colber is a smart Morson and a TSA Tatortainer.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "NEUTRAL = 0\n",
    "TOXIC = 1\n",
    "def paraphrase_and_rerank(text, max_length='auto', beams=10, rerank=False, clf=None, target=None):\n",
    "    def predict_toxicity(texts, clf):\n",
    "        with torch.inference_mode():\n",
    "            inputs = clf_tokenizer(texts, return_tensors='pt', padding=True)\n",
    "            out = torch.softmax(clf(**inputs).logits, -1)[:, 1].cpu().numpy()\n",
    "        return out\n",
    "\n",
    "    gedi_adapter = GediAdapter(\n",
    "        model=para_model, gedi_model=gedi_model, \n",
    "        tokenizer=tokenizer, \n",
    "        gedi_logit_coef=10, \n",
    "        target=target, \n",
    "        neg_code=NEW_NEG, \n",
    "        pos_code=NEW_POS, \n",
    "        reg_alpha=3e-5, ub=0.01\n",
    "    )\n",
    "\n",
    "    gen_texts = gedi_adapter.paraphrase(\n",
    "        text=text, max_length=max_length, beams=beams, \n",
    "        preprocess=text_processing.text_preprocess, \n",
    "        postprocess=text_processing.text_postprocess\n",
    "    )\n",
    "\n",
    "    if rerank:\n",
    "        toxic_scores = predict_toxicity(gen_texts)\n",
    "    \n",
    "    out_texts = []\n",
    "    for i in range(len(text)):\n",
    "        if rerank:\n",
    "            idx = toxic_scores[(i*beams):((i+1)*beams)].argmin()\n",
    "        else:\n",
    "            idx = 0 # Results ordered from most likely to least\n",
    "        out_texts.append(gen_texts[i*beams+idx])\n",
    "    return out_texts\n",
    "\n",
    "\n",
    "print(test_toxic_data[:5])\n",
    "paraphrase_and_rerank(text=test_toxic_data[:5], target=NEUTRAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15622e9d711615a40f6f9a03c4e5e10b8eada614db0f6bf16a21b6d90c59ac05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
